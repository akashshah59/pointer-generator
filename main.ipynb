{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose articles and save to a file\n",
    "\n",
    "rootdir = '../articles'\n",
    "allarticles = os.listdir(rootdir)\n",
    "\n",
    "num_train = 10000\n",
    "num_val = 5000\n",
    "total_articles = random.sample(allarticles, num_train + num_val)\n",
    "train_articles = total_articles[:num_train]\n",
    "val_articles = total_articles[num_train:]\n",
    "with open('train_files.txt', 'w') as f:\n",
    "    f.write('\\n'.join(train_articles))\n",
    "with open('val_files.txt', 'w') as f:\n",
    "    f.write('\\n'.join(val_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_workers = 4\n",
    "\n",
    "ARTICLE_MAX_TOKENS = 400\n",
    "SUMMARY_MAX_TOKENS = 100\n",
    "\n",
    "NUM_HIDDEN_STATES = 256\n",
    "EMBEDDING_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wikihowDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dtype):\n",
    "        \n",
    "        rootdir = '../articles'\n",
    "        if dtype == 'train':\n",
    "            with open('train_files.txt', 'r') as f:\n",
    "                paths = [os.path.join(rootdir,x.rstrip('\\n')) for x in f.readlines()]\n",
    "        elif dtype == 'val':\n",
    "            with open('val_files.txt', 'r') as f:\n",
    "                paths = [os.path.join(rootdir,x.rstrip('\\n')) for x in f.readlines()]\n",
    "        else:\n",
    "            print('invalid dtype of dataset', 1/0)\n",
    "        \n",
    "        self.articles = []\n",
    "        self.summaries = []\n",
    "        self.titles = []\n",
    "        self.vocab = set()\n",
    "        \n",
    "        for path in paths:\n",
    "            with open(path, 'r') as f:\n",
    "                summary = ''\n",
    "                article = ''\n",
    "                isarticle = False\n",
    "                \n",
    "                title = path.split('/')[2].split('.')[0]\n",
    "                self.titles.append(title)\n",
    "                \n",
    "                for line in [x.rstrip('\\n') for x in f.readlines()]:\n",
    "                    if line == '\\n' or line == '@summary':\n",
    "                        continue\n",
    "                        \n",
    "                    if line == '@article':\n",
    "                        isarticle = True\n",
    "                        continue\n",
    "                        \n",
    "                    if isarticle:\n",
    "                        article += line\n",
    "                    else:\n",
    "                        summary += line\n",
    "                        \n",
    "                article = ' '.join(article.split()[:ARTICLE_MAX_TOKENS])\n",
    "                summary = ' '.join(summary.split()[:SUMMARY_MAX_TOKENS])\n",
    "                self.articles.append(article)\n",
    "                self.summaries.append(summary)\n",
    "                \n",
    "        ### build vocabulary\n",
    "        for i in range(len(self.articles)):\n",
    "            for word in self.articles[i].split():\n",
    "                self.vocab.add(word)\n",
    "            for word in self.summaries[i].split():\n",
    "                self.vocab.add(word)\n",
    "                \n",
    "    def mapandpad(self, WORD_TO_IDX):\n",
    "        self.mappedarticles = [[WORD_TO_IDX[word] for word in article.split()] for article in self.articles]\n",
    "        self.mappedsummaries = [[WORD_TO_IDX[word] for word in summary.split()] for summary in self.summaries]\n",
    "        \n",
    "        pad_token = WORD_TO_IDX['<PAD>']\n",
    "        self.paddedarticles = np.ones((len(self.articles), ARTICLE_MAX_TOKENS)) * pad_token\n",
    "        self.paddedsummaries = np.ones((len(self.summaries), SUMMARY_MAX_TOKENS)) * pad_token\n",
    "        self.articlelengths = [len(ma) for ma in self.mappedarticles]\n",
    "        self.summarylengths = [len(ma) for ma in self.mappedsummaries]\n",
    "        \n",
    "        for i, x_len in enumerate(self.articlelengths):\n",
    "            sequence = self.mappedarticles[i]\n",
    "            self.paddedarticles[i, :x_len] = sequence[:x_len]\n",
    "            \n",
    "        for i, y_len in enumerate(self.summarylengths):\n",
    "            sequence = self.mappedsummaries[i]\n",
    "            self.paddedsummaries[i, :y_len] = sequence[:y_len]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.paddedarticles[idx], self.paddedsummaries[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunburn can cause cancer, blisters, cracking, and peeling.Use lip balm that contains sunscreen (at least 15 SPF), or a sunscreen on your lips. Use this every day to help protect your lips against the sun., Make sure your lip balm is free of anything you might be allergic to. This may cause an adverse reaction. Cleansers can contain chemicals such as salicylic acid or benzoyl peroxide which can cause dryness and, in some cases, allergic reactions., Cucumbers hold a lot of moisture and can be great for your lips.Consider a moisturizer that uses cucumber.You can also apply a cucumber directly for your lips to 3-5 minutes. This will allow your lips to soak up the moisture from the vegetable. Vitamin A and zinc are also important for you body. These vitamins help your immune system and help your skin stay healthy.\n",
      "[203932, 150102, 55293, 232246, 288391, 165640, 271178, 145359, 137733, 261860, 245503, 6885, 257926, 209830, 102862, 113669, 224374, 161621, 58004, 257926, 103889, 70326, 60979, 185769, 260865, 95002, 19897, 17258, 97260, 84351, 70326, 250102, 266834, 7611, 40216, 192297, 17450, 70326, 137733, 261860, 73890, 274903, 187483, 290659, 132657, 71805, 294898, 282027, 45374, 284946, 191535, 55293, 218524, 279536, 279823, 93543, 150102, 242367, 140898, 19575, 282963, 97549, 258956, 161621, 291757, 106445, 266701, 150102, 55293, 257958, 18729, 76657, 185991, 282556, 282027, 12202, 212575, 258452, 58004, 153797, 187483, 298938, 271178, 150102, 294898, 243238, 70701, 70326, 4198, 58004, 117560, 245503, 11178, 78392, 150102, 139984, 177400, 58004, 61145, 201405, 70701, 70326, 250102, 17258, 159968, 209259, 284946, 56728, 300598, 70326, 250102, 17258, 6315, 217962, 7611, 298938, 218470, 7611, 226203, 173624, 298311, 271178, 1376, 9572, 139984, 206594, 70701, 132657, 14956, 41440, 45643, 97260, 70326, 30202, 168744, 271178, 97260, 70326, 6976, 84789, 9844]\n",
      "[203932. 150102.  55293. 232246. 288391. 165640. 271178. 145359. 137733.\n",
      " 261860. 245503.   6885. 257926. 209830. 102862. 113669. 224374. 161621.\n",
      "  58004. 257926. 103889.  70326.  60979. 185769. 260865.  95002.  19897.\n",
      "  17258.  97260.  84351.  70326. 250102. 266834.   7611.  40216. 192297.\n",
      "  17450.  70326. 137733. 261860.  73890. 274903. 187483. 290659. 132657.\n",
      "  71805. 294898. 282027.  45374. 284946. 191535.  55293. 218524. 279536.\n",
      " 279823.  93543. 150102. 242367. 140898.  19575. 282963.  97549. 258956.\n",
      " 161621. 291757. 106445. 266701. 150102.  55293. 257958.  18729.  76657.\n",
      " 185991. 282556. 282027.  12202. 212575. 258452.  58004. 153797. 187483.\n",
      " 298938. 271178. 150102. 294898. 243238.  70701.  70326.   4198.  58004.\n",
      " 117560. 245503.  11178.  78392. 150102. 139984. 177400.  58004.  61145.\n",
      " 201405.  70701.  70326. 250102.  17258. 159968. 209259. 284946.  56728.\n",
      " 300598.  70326. 250102.  17258.   6315. 217962.   7611. 298938. 218470.\n",
      "   7611. 226203. 173624. 298311. 271178.   1376.   9572. 139984. 206594.\n",
      "  70701. 132657.  14956.  41440.  45643.  97260.  70326.  30202. 168744.\n",
      " 271178.  97260.  70326.   6976.  84789.   9844. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227.]\n",
      "400\n",
      "Access the Music application and wait for it to load. Make sure you're holding the iPod vertically. Then, press 'Now Playing' like a boss. Slide it to the right.\n",
      "[69513, 7611, 131067, 32373, 271178, 117768, 70701, 231136, 17258, 39010, 192297, 17450, 277198, 250062, 7611, 31119, 99979, 195213, 240403, 88842, 129913, 125567, 58004, 4714, 291409, 231136, 17258, 7611, 273121]\n",
      "[ 69513.   7611. 131067.  32373. 271178. 117768.  70701. 231136.  17258.\n",
      "  39010. 192297.  17450. 277198. 250062.   7611.  31119.  99979. 195213.\n",
      " 240403.  88842. 129913. 125567.  58004.   4714. 291409. 231136.  17258.\n",
      "   7611. 273121. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227. 243227.\n",
      " 243227. 243227. 243227. 243227.]\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "train_dataset = wikihowDataset('train')\n",
    "val_dataset = wikihowDataset('val')\n",
    "\n",
    "vocab = train_dataset.vocab | val_dataset.vocab\n",
    "vocab.add('<PAD>')\n",
    "vocab = list(vocab)\n",
    "WORD_TO_IDX = {word: i for i, word in enumerate(vocab)}\n",
    "PADDING_IDX = WORD_TO_IDX['<PAD>']\n",
    "\n",
    "train_dataset.mapandpad(WORD_TO_IDX)\n",
    "val_dataset.mapandpad(WORD_TO_IDX)\n",
    "\n",
    "print(train_dataset.articles[0])\n",
    "print(train_dataset.mappedarticles[0])\n",
    "print(train_dataset.paddedarticles[0])\n",
    "print(len(train_dataset.paddedarticles[0]))\n",
    "\n",
    "print(val_dataset.articles[0])\n",
    "print(val_dataset.mappedarticles[0])\n",
    "print(val_dataset.paddedarticles[0])\n",
    "print(len(val_dataset.paddedarticles[0]))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,)\n",
      "(10000, 400)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.paddedarticles[0].shape)\n",
    "print(train_dataset.paddedarticles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=len(vocab), \n",
    "            embedding_dim=EMBEDDING_SIZE, \n",
    "            padding_idx=PADDING_IDX\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=NUM_HIDDEN_STATES, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, x_lengths):\n",
    "        # x (400, batch_size, 1)\n",
    "        \n",
    "        x = self.embeddings(x)\n",
    "        # (400, batch_size, 128)\n",
    "        \n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, x_lengths)\n",
    "        \n",
    "        x = self.lstm(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
